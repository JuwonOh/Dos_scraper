{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get briefing statement urls 2018 / 2019\n",
      "1790 urls for all news\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import DoS_scraper\n",
    "from DoS_scraper import get_allnews_urls\n",
    "from DoS_scraper import parse_page\n",
    "\n",
    "DoS_urls = get_allnews_urls(begin_year=2018, end_year=2019, verbose=True)\n",
    "print('%d urls for all news' % len(DoS_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "def now():\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    Current time : str\n",
    "        eg: 2018-11-22 13:35:23\n",
    "    \"\"\"\n",
    "    return strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "def get_soup(url, headers=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    url : str\n",
    "        Web page url\n",
    "    headers : dict\n",
    "        Headers for requests. If None, use Mozilla/5.0 as default user-agent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    soup : bs4.BeautifulSoup\n",
    "        Soup format web page\n",
    "    \"\"\"\n",
    "\n",
    "    if headers is None:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    html = r.text\n",
    "    page = BeautifulSoup(html, 'lxml')\n",
    "    return page\n",
    "\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "lineseparator_pattern = re.compile('\\n+')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = lineseparator_pattern.sub('\\n', text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "def get_latest_allnews(last_date, sleep=1.0):\n",
    "    \"\"\"\n",
    "    Artuments\n",
    "    ---------\n",
    "    last_date : Date\n",
    "    sleep : float\n",
    "        Sleep time. Default 1.0 sec\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplemented\n",
    "\n",
    "patterns_transcript = [\n",
    "    re.compile('https://www.state.gov/[\\w]+')]\n",
    "base_url = 'https://www.state.gov/r/pa/prs/ps/{}/index.htm'\n",
    "\n",
    "def get_allnews_urls(begin_year=2018, end_year=2019, verbose=True):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    begin_page : int\n",
    "        Default is 1\n",
    "    end_page : int\n",
    "        Default is 3\n",
    "    verbose : Boolean\n",
    "        If True, print current status\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    links_all : list of str\n",
    "        List of urls\n",
    "    \"\"\"\n",
    "\n",
    "    links_all = []\n",
    "    for year in range(begin_year, end_year):\n",
    "        url = base_url.format(year)\n",
    "        soup = get_soup(url)\n",
    "        sub_links = soup.find('div', class_= 'l-wrap')\n",
    "        for link in sub_links.find_all(\"a\"):\n",
    "            if 'href' in link.attrs:\n",
    "                 links_all += [link.attrs['href']]\n",
    "        if verbose:\n",
    "            print('get briefing statement urls {} / {}'.format(begin_year, end_year))\n",
    "\n",
    "    links_all = ['https://www.state.gov' + i for i in links_all]\n",
    "\n",
    "    return links_all\n",
    "\n",
    "def get_last_page_num():\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    page : int\n",
    "        Last page number.\n",
    "        eg: 503 in 'https://dod.defense.gov/News/Transcripts/?Page=62'\n",
    "    \"\"\"\n",
    "    raise NotImplemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import gmtime, strftime\n",
    "\n",
    "def parse_page(url):\n",
    "    \"\"\"\n",
    "    Argument\n",
    "    --------\n",
    "    url : str\n",
    "        Web page url\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    json_object : dict\n",
    "        JSON format web page contents\n",
    "        It consists with\n",
    "            title : article title\n",
    "            time : article written time\n",
    "            content : text with line separator \\\\n\n",
    "            url : web page url\n",
    "            scrap_time : scrapped time\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "        title = soup.find('h2', class_= 'title left').text\n",
    "        time = soup.find('div', id= 'date_long').text\n",
    "        content = soup.find('div', id = 'centerblock').text\n",
    "\n",
    "        json_object = {\n",
    "            'title' : title,\n",
    "            'time' : time,\n",
    "            'content' : content,\n",
    "            'url' : url,\n",
    "            'scrap_time' : now()\n",
    "        }\n",
    "        return json_object\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Parsing error from {}'.format(url))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : Attack in Giza ..\n",
      "time : December 28, 2018 ..\n",
      "content : \n",
      "The United States strongly condemns the attack carried out today on a tourist bus in Giza. Our deep ..\n",
      "url : https://www.state.gov/r/pa/prs/ps/2018/12/288285.htm ..\n",
      "scrap_time : 2019-01-13 23:49:58 ..\n",
      "\n",
      "\n",
      "title : Israel's Right to Self-Defense ..\n",
      "time : December 28, 2018 ..\n",
      "content : \n",
      "The United States fully supports Israelâ€™s right to defend itself against Iranian regional actions t ..\n",
      "url : https://www.state.gov/r/pa/prs/ps/2018/12/288282.htm ..\n",
      "scrap_time : 2019-01-13 23:49:58 ..\n",
      "\n",
      "\n",
      "title : Secretary Pompeo Travels to Brazil and Colombia To Strengthen Prosperity, Security, and Democracy ..\n",
      "time : December 28, 2018 ..\n",
      "content : \n",
      "Secretary Pompeo is leading a Presidential Delegation to Brazil for the inauguration of President-e ..\n",
      "url : https://www.state.gov/r/pa/prs/ps/2018/12/288281.htm ..\n",
      "scrap_time : 2019-01-13 23:49:58 ..\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pprint(json_object):\n",
    "    for k, v in json_object.items():\n",
    "        print('{} : {} ..'.format(k, str(v)[:100]))\n",
    "    print('\\n')\n",
    "\n",
    "SLEEP = 0.5\n",
    "\n",
    "for url in DoS_urls[:3]:\n",
    "    json_object = parse_page(url)\n",
    "    pprint(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.545454,
   "position": {
    "height": "144px",
    "left": "1021.45px",
    "right": "20px",
    "top": "89px",
    "width": "249px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
